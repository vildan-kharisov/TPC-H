# TPC-H

#### Цель проекта: 
Получить практические навыки и представление о совместной работе DWH, Data Lake и Data Orchestrator на примере Greenplum | Spark | Airflow.

TPC-H - это стандартный бенчмарк для тестирования производительности систем управления реляционными базами данных (RDBMS). Он состоит из набора запросов, которые используются для оценки производительности обработки запросов на чтение больших объемов данных из базы данных.

TPC-H был разработан Transaction Processing Performance Council (TPC) - некоммерческой организацией, которая занимается разработкой стандартов и методов для тестирования производительности систем обработки транзакций. Бенчмарк состоит из 22 запросов, которые покрывают широкий диапазон типов запросов, таких как агрегирование, слияние таблиц и фильтрация данных.

TPC-H используется для тестирования производительности RDBMS на больших объемах данных, таких как решения хранилища данных и системы аналитики. Он помогает выявлять узкие места в производительности системы и оптимизировать её работу при работе с большими объёмами данных. Результаты тестирования TPC-H используются для сравнения производительности разных систем RDBMS.

#### Состав проекта: 
- <a href="https://github.com/vildan-kharisov/TPC-H/tree/main/SPARK-job" target="_blank">пять запросов на Spark</a>, объединенных в один DAG Airflow
- <a href="https://github.com/vildan-kharisov/TPC-H/tree/main/Airflow-pipeline" target="_blank">пять запросов на Greenplum</a>, объединенных в один DAG Airflow

### Образовательные результаты проекта: 
- использование изученных инструментов в связке: Airflow, Spark, Greenplum;
- понимание, как строятся пайплайны обработки данных в озере данных и DWH;
- построение отчётов DE по ТЗ от заказчика.

#### Задачи, решенные в ходе проекта:
- Спроектировал классическую логическую архитектуру, представленную в DAMA DMBOOK в виде аналитического хранилища (analytical data) и озера данных (data lake);
- Для целей создания технической инфраструктуры использовал следующие технологии:
* Аналитическое хранилище на основе Greenplum;
* Озеро данных на основе технологии S3;
* Распределенные вычисление на основе Spark
* Оркестрация потоков на основе Airflow;
* Хранение исходного кода в GitLab.
- Произвел интеграцию Greenplum и Spark с использованием S3;
- Создал скрипты для Spark Job по обработке данных, полученных с внешних источников в Stage-слое S3;
- Наладил выгрузку, сформировал и заполнил данными хранилище S3 из внешнего источника;
- Сформировал необходимые витрины в Greenplum с помощью технологии Greenplum PXF над отчетами из внешнего хранилища S3;
- В Airflow создал необходимые DAG-и для отправки Spark-задач на кластер, отслеживания задач Spark и их статуса, для выполнения запросов к базе Greenplum;
